# MM-UAVBench 任务描述与所运用模型、方法

本文档给出**各任务的详细描述**以及本复现中**所运用的模型与评测方法**。

---

## 一、任务详细描述

MM-UAVBench 将低空无人机场景下的多模态能力分为三个维度：**感知（Perception）**、**认知（Cognition）**、**规划（Planning）**。以下为 16 个**图像任务**的逐一说明。

### 1. 感知层（Perception）

| 任务英文名 | 任务描述 | 典型问题与选项形式 | 能力与备注 |
|------------|----------|--------------------|------------|
| **Scene_Classification** | 场景分类 | 给定（可选）边界框或整图，问“该区域/图像的主要场景类型是什么”。选项为具体场景名，如：Basketball court / Soccer field / Traffic road / Parking lot / Residential buildings 等。 | 考察对航拍场景的语义识别（场地、道路、建筑、水域等）。部分题目带 bbox，需理解“红框内”所指区域。 |
| **Orientation_Classification** | 朝向/运动方向分类 | 给定某车辆（或目标）的边界框，问“该目标相对自身朝向的即时运动方向”。选项：Moving straight / Turning right / Turning left / Stationary。 | 考察从单帧图像推断运动趋势或朝向。单帧无时序信息，CLIP 零样本在此任务上表现通常较弱。 |
| **Environment_State_Classification** | 环境状态分类 | 问“图像中的光照与天气条件是什么”。选项为「光照 + 天气」组合，如：daytime and cloudy / nighttime and clear / dusk and sunny / daytime and rainy 等。 | 考察对光照（白天/黄昏/夜晚等）与天气（晴/阴/雨/雾等）的识别，与场景语义相对解耦，CLIP 零样本较适用。 |
| **Urban_OCR** | 城市场景文字识别 | 针对航拍图像中的文字（标牌、路牌、建筑标识等），问与文字内容相关的选择题。选项多为文字或与文字含义相关的描述。 | 考察视觉语言联合理解与 OCR 能力；CLIP 非专门 OCR 模型，对细粒度文字敏感度有限。 |
| **Class_Agnostic_Counting** | 类别无关计数 | 问图像中某类或某区域内的物体数量（或数量区间）。选项为数字或区间，如：3 / 5 / 7 / 10 等。 | 考察目标计数能力，不限定具体类别；需从图像中推理数量并与选项对应。 |
| **Referring_Expression_Counting** | 指代表达计数 | 通过自然语言指代（如“红色车辆”“左侧建筑物”）问数量，选项为数字或描述。 | 结合指代理解与计数，对语言-区域对齐要求较高。 |

### 2. 认知层（Cognition）

| 任务英文名 | 任务描述 | 典型问题与选项形式 | 能力与备注 |
|------------|----------|--------------------|------------|
| **Target_Backtracking** | 目标回溯 | 根据当前画面或给定线索，问“目标此前可能来自哪一区域/哪条路径”等回溯类问题。选项为位置或路径描述。 | 考察时空推理与轨迹回溯，常需多帧或先验；单图 CLIP 仅能利用当前画面做有限推理。 |
| **Cross_Object_Reasoning** | 跨物体推理 | 问多个物体之间的关系、比较或基于多目标的推断（如相对位置、谁更靠近某地等）。选项为关系或结论描述。 | 考察多实体关系与比较推理，依赖目标检测与关系建模。 |
| **Intent_Analysis_and_Prediction** | 意图分析与预测 | 问图中某主体（车、人、无人机等）的意图或接下来可能的行为。选项为意图或行为描述。 | 考察从静态画面推断意图与行为预测，与 Orientation 类似，单帧信息有限。 |
| **Scene_Attribute_Understanding** | 场景属性理解 | 问场景的抽象属性（如功能、用途、危险程度、是否适合起降等）。选项为属性或判断描述。 | 考察对场景的高层语义与属性理解，CLIP 的语义空间可部分覆盖。 |
| **Scene_Damage_Assessment** | 场景损毁评估 | 问灾害或事故后场景的损毁程度、类型或影响。选项为损毁等级或描述。 | 考察灾害/损毁相关视觉理解，需区分正常与异常、不同程度损毁。 |
| **Scene_Analysis_and_Prediction** | 场景分析与预测 | 问对当前场景的合理推断或“接下来可能发生什么”。选项为事件或状态描述。 | 结合场景理解与简单因果/时序预测，单图下多为“最可能情况”选择。 |
| **Temporal_Ordering** | 时序排序 | 可能给出多图或单图中的多个时刻/状态，问事件顺序或先后关系。选项为顺序描述。 | 考察时序理解；若题目为多图，需多图联合编码，本复现当前为单图 CLIP，仅能利用题目中的单张图。 |

### 3. 规划层（Planning）

| 任务英文名 | 任务描述 | 典型问题与选项形式 | 能力与备注 |
|------------|----------|--------------------|------------|
| **Ground_Target_Planning** | 地面目标规划 | 问如何对地面目标执行任务（如巡检顺序、搜救路径、观测点位等）。选项为规划方案或策略描述。 | 考察在给定场景下生成或选择可行规划，需理解场景几何与任务约束。 |
| **Air_Ground_Collaborative_Planning** | 空地协同规划 | 问无人机与地面单元（车、人）如何协同完成某任务。选项为协同策略或分工描述。 | 考察多智能体协同与任务分解，依赖对场景与角色能力的理解。 |
| **Swarm_Collaborative_Planning** | 集群协同规划 | 问多架无人机（或多机与地面）如何协作完成任务。选项为编队、分工或协同方案。 | 考察多机/集群规划，选项多为抽象策略，CLIP 从图像-文本匹配角度做选择。 |

---

## 二、所运用的模型（多模型支持）

本复现支持**多种视觉-语言模型**，统一为“图像 + 选项文本 → 相似度 → argmax 选项”的零样本 MCQ 接口，可同时跑多个模型并生成对比报告。

| 模型 id | 模型名称 | 权重来源 | 说明 |
|---------|----------|----------|------|
| **clip_vitb32** | CLIP ViT-B/32 | `openai/clip-vit-base-patch32` | 轻量、速度快，适合基线对比。 |
| **siglip_base** | SigLIP Base 224 | `google/siglip-base-patch16-224` | 使用 sigmoid 损失的对比学习，零样本分类常优于同规模 CLIP。 |
| **clip_vitl14** | CLIP ViT-L/14 | `openai/clip-vit-large-patch14` | 更大 backbone，表达能力更强，显存/算力需求更高。 |
| **qwen2vl_2b** | Qwen2-VL-2B-Instruct | `Qwen/Qwen2-VL-2B-Instruct` | 生成式 VLM，根据图像+题目生成选项字母，适合复杂推理。 |
| **qwen2vl_7b** | Qwen2-VL-7B-Instruct | `Qwen/Qwen2-VL-7B-Instruct` | 同上，7B 参数，需更多显存。 |

**共同约定**：

- **输入**：单张 RGB 图像（经各模型对应的预处理）；选项为英文原文。
- **输出**：图像与每个选项的相似度（logits）；**预测 = argmax 对应选项字母**。
- **调用方式**：命令行 `--models clip_vitb32 siglip_base` 可指定多个模型，脚本会依次加载、对同一批任务逐题推理，并在报告中对各模型分别统计并输出**任务×模型**汇总表。

选择多模型的原因：

- **零样本**：所有模型均不做任务微调，便于公平对比不同 VLM 在 MM-UAVBench 上的表现。
- **统一接口**：所有任务均为“一图 + 多选项” MCQ，同一套流程适用于 CLIP 与 SigLIP。
- **可复现**：权重与预处理均来自 HuggingFace `transformers`，便于复现与扩展（如增加更多 CLIP/SigLIP 变体）。

---

## 三、所运用的方法

### 3.1 评测流程（方法概览）

1. **数据来源**：从 HuggingFace 数据集 `daisq/MM-UAVBench` 拉取官方 `tasks/*.json` 及对应 `images/annotated/...` 图像。
2. **题目形式**：每题包含一张航拍图像、一道英文选择题、若干选项（A/B/C/D 等）及标准答案（单字母）。
3. **推理方式**：对每题，将**整张图像**与**每个选项的文本**输入 CLIP，得到图像-选项相似度向量，取 **argmax** 作为预测选项字母。
4. **评估指标**：**Exact Match（精确匹配）**——预测选项与标准答案字母一致即判对；统计每题正确与否，汇报各任务及总体的**正确数/总题数**与**准确率（%）**。
5. **报告输出**：将上述结果写入 `results/MM-UAVBench_report.txt`（或通过 `--report-path` 指定路径）。

### 3.2 形式化表述

- 设单题图像为 \(x\)，选项集合为 \(\{(c_1, L_1), (c_2, L_2), \ldots, (c_K, L_K)\}\)，其中 \(c_k\) 为选项字母，\(L_k\) 为选项文本。
- CLIP 输出图像编码 \(f_{\text{img}}(x)\) 与文本编码 \(f_{\text{txt}}(L_k)\)，相似度（如余弦或点积）为 \(\mathrm{sim}(x, L_k)\)。
- **预测**：\(\hat{c} = c_{\arg\max_k \mathrm{sim}(x, L_k)}\)。
- **判对**：若 \(\hat{c}\) 等于官方答案字母，则该题正确。

### 3.3 实现细节（与本仓库一致)

- **图像**：使用官方标注图像（含已绘制 bbox 的图）；当前实现**不单独做 bbox 裁剪**，整图送入 CLIP，以便所有任务共用同一管线。
- **文本**：直接使用题目中 `options` 的英文原文，不做改写或扩展。
- **选项顺序**：按选项字母（A/B/C/D/…）排序后与 CLIP 输出 logits 一一对应，argmax 下标映射回字母即为预测。
- **设备**：支持 CPU 与 CUDA；无额外后处理或集成多模型。

### 3.4 方法局限（相对 MM-UAVBench 设计意图）

- **单图、无时序**：Orientation、Intent、Target_Backtracking、Temporal_Ordering 等任务设计上可能依赖多帧或视频，当前仅用单帧，CLIP 零样本表现会受限。
- **无 bbox 裁剪**：Scene_Classification 等题目中的 bbox 未用于裁剪，模型看到的是整图，可能弱化“红框内”的聚焦效果。
- **纯零样本**：未使用任务特定数据训练或提示学习，仅作为**基线方法**与论文中的 MLLM/VLM 结果对比。

---

## 四、硬件检测与优化

运行前可执行 `python run_mmuavbench_official_tasks.py --check-hardware` 查看本机 CPU 核数、是否可用 GPU、内存等。

脚本会根据硬件自动做部分优化：

- **CPU 模式**：自动设置 PyTorch 线程数为 `min(16, cpu_count)`，并设置 `OMP_NUM_THREADS` 以减轻过度订阅；可通过 `--cpu-threads N` 覆盖。
- **Qwen2-VL 在 CPU 下**：使用 `low_cpu_mem_usage=True` 加载；图像最大像素限制为 336×336（GPU 为 512×512）；生成最大 token 数设为 16（GPU 为 32），以加快单题推理并控制内存。
- **通用**：推理阶段使用 `torch.inference_mode()`；启动时设置 `HF_HUB_DISABLE_SYMLINKS_WARNING=1`、`TRANSFORMERS_VERBOSITY=error` 以减少控制台刷屏。

无 GPU 时建议仅跑轻量模型（如 `--models clip_vitb32 siglip_base`），或对 Qwen2-VL 使用较小 `--max-samples` 与 `--cpu-threads`。

**AMD 显卡**：脚本通过 `torch.cuda` 使用 GPU，ROCm 版 PyTorch 会同样暴露为 CUDA API。安装 AMD 官方的 [ROCm 版 PyTorch](https://rocm.docs.amd.com/projects/radeon-ryzen/en/latest/docs/install/installrad/windows/install-pytorch.html)（Windows 需按文档装 ROCm SDK + 对应 whl）后，无需改代码即可用 AMD GPU 跑评测。

**加速实验**：使用 `--fast` 可启用快速预设（每任务 10 题、仅 clip_vitb32 + siglip_base，并自动设置 CPU 线程）；GPU 下会开启 cuDNN benchmark，CPU 下会尝试设置 `torch.set_float32_matmul_precision("medium")` 以略加速矩阵运算。适合快速验证流程或出结果。详见 README「怎么跑」中的 `--fast`。

## 五、与报告文件的对应关系

**多模型时**，`results/MM-UAVBench_report.txt` 结构为：

1. **表头**：生成时间、设备、每任务样本数、任务数、模型数。
2. **按模型分块**：每个模型一块，块内每行形如  
   `[Task_Name] 正确: correct/total  准确率: acc%`  
   块尾为 `>> 总体: correct/total  平均准确率: acc%`。
3. **汇总表**：表格行为各任务（及“总体”行），列为各模型 id，单元格为该任务在该模型下的准确率（%）。

单模型时仍为“按模型分块”中一块的格式。

本文档中的**任务描述**与**模型、方法**即对应生成该报告时所用的任务定义与评测流程。
